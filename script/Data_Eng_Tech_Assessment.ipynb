{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI4738X+MYtcnlgARSgscQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-vipriy/data-eng-tech-assessment/blob/master/script/Data_Eng_Tech_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/v-vipriy/data-eng-tech-assessment/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzwR5LvRGlNP",
        "outputId": "21ac365e-04fd-43f0-e0a4-aa6761571d27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'data-eng-tech-assessment'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 75 (delta 23), reused 57 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), 2.34 MiB | 3.78 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSwMaBh_kb4d"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from apache_beam.options.pipeline_options import PipelineOptions\n",
        "  import   apache_beam as beam\n",
        "  import urllib.request\n",
        "  import json\n",
        "except:\n",
        "  ! pip   install apache-beam[interactive]\n",
        "  from apache_beam.options.pipeline_options import PipelineOptions\n",
        "  import   apache_beam as beam\n",
        "  import urllib.request\n",
        "  import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class load_json_as_dictionary(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    result = [{k: int(v) if k == 'locationid' else v for k, v in d.items()} for d in map(lambda x: dict(x), json.loads(element))]\n",
        "    return result\n",
        "\n",
        "class select_elements_location_data(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    result = [{k: v for k, v in element.items() if k in ('location_id', 'sensor_description','sensor_name')}]\n",
        "    return result\n",
        "\n",
        "class drop_redundant_fields(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    result = [{k: v for k, v in element.items() if k not in ('location_id')}]\n",
        "    return result\n",
        "\n",
        "class LeftJoin(beam.PTransform):\n",
        "    \"\"\"This PTransform performs a left join given source_pipeline_name, source_data,\n",
        "     join_pipeline_name, join_data, common_key constructors\"\"\"\n",
        "\n",
        "    def __init__(self, source_pipeline_name, source_data, join_pipeline_name, join_data, left_key,right_key):\n",
        "        self.join_pipeline_name = join_pipeline_name\n",
        "        self.source_data = source_data\n",
        "        self.source_pipeline_name = source_pipeline_name\n",
        "        self.join_data = join_data\n",
        "        self.left_key = left_key\n",
        "        self.right_key = right_key\n",
        "        # print(\"Inside LeftJoin -> __init))\")\n",
        "\n",
        "    def expand(self, pcolls):\n",
        "        # print(\"Inside LeftJoin -> expand : 1\")\n",
        "        def _format_as_common_key_tuple(data_dict, left_key,right_key):\n",
        "            # print(\"Inside LeftJoin -> expand : map(data_dict[common_key],int)\")\n",
        "            # print(type(data_dict), type(common_key))\n",
        "            # print(data_dict)\n",
        "            try:\n",
        "              # print(\"data_dict[left_key] \", data_dict[left_key])\n",
        "              return data_dict[left_key], data_dict\n",
        "            except:\n",
        "              # print(\"data_dict[right_key] \", data_dict[right_key])\n",
        "              return data_dict[right_key], data_dict\n",
        "\n",
        "        # print(\"Inside LeftJoin -> expand : 2\")\n",
        "        return ({pipeline_name: pcoll | 'Convert to ({0}/{1}, object) for {2}'\n",
        "                .format(self.left_key,self.right_key, pipeline_name)\n",
        "                                >> beam.Map(_format_as_common_key_tuple, self.left_key, self.right_key)\n",
        "                 for (pipeline_name, pcoll) in pcolls.items()}\n",
        "                | 'CoGroupByKey {0}'.format(pcolls.keys()) >> beam.CoGroupByKey()\n",
        "                | 'Unnest Cogrouped' >> beam.ParDo(UnnestCoGrouped(),\n",
        "                                                   self.source_pipeline_name,\n",
        "                                                   self.join_pipeline_name)\n",
        "                )\n",
        "\n",
        "class UnnestCoGrouped(beam.DoFn):\n",
        "    \"\"\"This DoFn class unnests the CogroupBykey output and emits \"\"\"\n",
        "\n",
        "    def process(self, input_element, source_pipeline_name, join_pipeline_name):\n",
        "        group_key, grouped_dict = input_element\n",
        "        join_dictionary = grouped_dict[join_pipeline_name]\n",
        "        source_dictionaries = grouped_dict[source_pipeline_name]\n",
        "        for source_dictionary in source_dictionaries:\n",
        "            try:\n",
        "                source_dictionary.update(join_dictionary[0])\n",
        "                yield source_dictionary\n",
        "            except IndexError:  # found no join_dictionary\n",
        "                yield source_dictionary\n",
        "\n",
        "def run(argv=None):\n",
        "    \"\"\"Main entry point\"\"\"\n",
        "    pedestrian_data_url = 'https://data.melbourne.vic.gov.au/api/v2/catalog/datasets/pedestrian-counting-system-monthly-counts-per-hour/exports/json'\n",
        "    location_data_url = 'https://data.melbourne.vic.gov.au/api/v2/catalog/datasets/pedestrian-counting-system-sensor-locations/exports/json'\n",
        "\n",
        "    output_location = '/content/data-eng-tech-assessment/output/location_output_data.txt'\n",
        "\n",
        "\n",
        "    # Download the file from `url`, save it in a temporary directory and get the path to it in the `file_name` variable:\n",
        "    pedestrian_data_file, pedestrian_data_headers = urllib.request.urlretrieve(pedestrian_data_url)\n",
        "    location_data_file, location_data_headers = urllib.request.urlretrieve(location_data_url)\n",
        "    left_key = 'locationid'\n",
        "    right_key = 'location_id'\n",
        "\n",
        "    pipeline_options = PipelineOptions()\n",
        "    p = beam.Pipeline(options=pipeline_options)\n",
        "\n",
        "    # Read data\n",
        "    pedestrian_pipeline_name = 'pedestrian_data'\n",
        "    pedestrian_data = (p | 'Read pedestrian data' >> beam.io.ReadFromText(pedestrian_data_file)\n",
        "                          | 'JSON_to_Dict pedestrian data' >> beam.ParDo(load_json_as_dictionary())\n",
        "                      )\n",
        "    location_pipeline_name = 'location_data'\n",
        "    location_data = (p | 'Read location data' >> beam.io.ReadFromText(location_data_file)\n",
        "                        | 'JSON_to_Dict location data' >> beam.ParDo(load_json_as_dictionary())\n",
        "                        | 'Select fields location data' >> beam.ParDo(select_elements_location_data())\n",
        "                    )\n",
        "\n",
        "    pipelines_dictionary = {pedestrian_pipeline_name: pedestrian_data,\n",
        "                            location_pipeline_name: location_data}\n",
        "    # print(\"Inside run : pipelines_dictionary\")\n",
        "    # print(pipelines_dictionary)\n",
        "    tranformation_pipeline = (pipelines_dictionary\n",
        "                     | 'Left join' >> LeftJoin(\n",
        "                pedestrian_pipeline_name, pedestrian_data,\n",
        "                location_pipeline_name, location_data, left_key,right_key)\n",
        "                    )\n",
        "\n",
        "    output  = ( tranformation_pipeline\n",
        "                | 'Cleanup' >> beam.ParDo(drop_redundant_fields())\n",
        "                | 'Format Output' >> beam.Map(json.dumps)\n",
        "                |beam.io.WriteToText(output_location)\n",
        "              )\n",
        "\n",
        "    result = p.run()\n",
        "    result.wait_until_finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "9b4YDi6jw7Q4",
        "outputId": "35540e85-bde7-4440-8ef4-2e0da564f417"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-7e0d0bd5-4b46-4de1-85dd-c7fa62854256.json']\n"
          ]
        }
      ]
    }
  ]
}