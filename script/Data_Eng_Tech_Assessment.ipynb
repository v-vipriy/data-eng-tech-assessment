{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GaT8p05bN-NbPqiq8yaWTfvgC2Wj6V4i",
      "authorship_tag": "ABX9TyNoj/aVG1llcukaCUfr45mi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-vipriy/data-eng-tech-assessment/blob/Vpriya_assignment/script/Data_Eng_Tech_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XSwMaBh_kb4d"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from apache_beam.options.pipeline_options import PipelineOptions\n",
        "  import   apache_beam as beam\n",
        "  import urllib.request\n",
        "  import logging\n",
        "  import json\n",
        "except:\n",
        "  ! pip   install apache-beam[interactive]\n",
        "  from apache_beam.options.pipeline_options import PipelineOptions\n",
        "  import   apache_beam as beam\n",
        "  import urllib.request\n",
        "  import logging\n",
        "  import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class load_json_as_dictionary(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    result = [{k: int(v) if k == 'locationid' else v for k, v in d.items()} for d in map(lambda x: dict(x), json.loads(element))]\n",
        "    return result\n",
        "\n",
        "class select_elements_location_data(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # print(type(element))\n",
        "    result = [{k: v for k, v in element.items() if k in ('location_id', 'sensor_description','sensor_name')}]\n",
        "    return result\n",
        "\n",
        "class drop_redundant_fields(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    result = [{k: v for k, v in element.items() if k not in ('location_id')}]\n",
        "    return result\n",
        "\n",
        "class LeftJoin(beam.PTransform):\n",
        "    \"\"\"This PTransform performs a left join given source_pipeline_name, source_data,\n",
        "     join_pipeline_name, join_data, common_key constructors\"\"\"\n",
        "\n",
        "    def __init__(self, source_pipeline_name, source_data, join_pipeline_name, join_data, left_key,right_key):\n",
        "        self.join_pipeline_name = join_pipeline_name\n",
        "        self.source_data = source_data\n",
        "        self.source_pipeline_name = source_pipeline_name\n",
        "        self.join_data = join_data\n",
        "        self.left_key = left_key\n",
        "        self.right_key = right_key\n",
        "        # print(\"Inside LeftJoin -> __init))\")\n",
        "\n",
        "    def expand(self, pcolls):\n",
        "        # print(\"Inside LeftJoin -> expand : 1\")\n",
        "        def _format_as_common_key_tuple(data_dict, left_key,right_key):\n",
        "            # print(\"Inside LeftJoin -> expand : map(data_dict[common_key],int)\")\n",
        "            # print(type(data_dict), type(common_key))\n",
        "            # print(data_dict)\n",
        "            try:\n",
        "              # print(\"data_dict[left_key] \", data_dict[left_key])\n",
        "              return data_dict[left_key], data_dict\n",
        "            except:\n",
        "              # print(\"data_dict[right_key] \", data_dict[right_key])\n",
        "              return data_dict[right_key], data_dict\n",
        "\n",
        "        # print(\"Inside LeftJoin -> expand : 2\")\n",
        "        return ({pipeline_name: pcoll | 'Convert to ({0}/{1}, object) for {2}'\n",
        "                .format(self.left_key,self.right_key, pipeline_name)\n",
        "                                >> beam.Map(_format_as_common_key_tuple, self.left_key, self.right_key)\n",
        "                 for (pipeline_name, pcoll) in pcolls.items()}\n",
        "                | 'CoGroupByKey {0}'.format(pcolls.keys()) >> beam.CoGroupByKey()\n",
        "                | 'Unnest Cogrouped' >> beam.ParDo(UnnestCoGrouped(),\n",
        "                                                   self.source_pipeline_name,\n",
        "                                                   self.join_pipeline_name)\n",
        "                )\n",
        "\n",
        "class UnnestCoGrouped(beam.DoFn):\n",
        "    \"\"\"This DoFn class unnests the CogroupBykey output and emits \"\"\"\n",
        "\n",
        "    def process(self, input_element, source_pipeline_name, join_pipeline_name):\n",
        "        group_key, grouped_dict = input_element\n",
        "        join_dictionary = grouped_dict[join_pipeline_name]\n",
        "        source_dictionaries = grouped_dict[source_pipeline_name]\n",
        "        for source_dictionary in source_dictionaries:\n",
        "            try:\n",
        "                source_dictionary.update(join_dictionary[0])\n",
        "                yield source_dictionary\n",
        "            except IndexError:  # found no join_dictionary\n",
        "                yield source_dictionary\n",
        "\n",
        "class LogContents(beam.DoFn):\n",
        "    \"\"\"This DoFn class logs the content of that which it receives \"\"\"\n",
        "\n",
        "    def process(self, input_element):\n",
        "        logging.info(\"Contents: {}\".format(input_element))\n",
        "        logging.info(\"Contents type: {}\".format(type(input_element)))\n",
        "        # logging.info(\"Contents Access input_element['Country']: {}\".format(input_element['Country']))\n",
        "        return\n",
        "\n",
        "def run(argv=None):\n",
        "    \"\"\"Main entry point\"\"\"\n",
        "    pedestrian_data_url = 'https://data.melbourne.vic.gov.au/api/v2/catalog/datasets/pedestrian-counting-system-monthly-counts-per-hour/exports/json'\n",
        "    location_data_url = 'https://data.melbourne.vic.gov.au/api/v2/catalog/datasets/pedestrian-counting-system-sensor-locations/exports/json'\n",
        "\n",
        "    # Download the file from `url`, save it in a temporary directory and get the path to it in the `file_name` variable:\n",
        "    pedestrian_data_file, pedestrian_data_headers = urllib.request.urlretrieve(pedestrian_data_url)\n",
        "    location_data_file, location_data_headers = urllib.request.urlretrieve(location_data_url)\n",
        "    left_key = 'locationid'\n",
        "    right_key = 'location_id'\n",
        "\n",
        "    pipeline_options = PipelineOptions()\n",
        "    p = beam.Pipeline(options=pipeline_options)\n",
        "\n",
        "    # Create Example read Dictionary data\n",
        "    pedestrian_pipeline_name = 'pedestrian_data'\n",
        "    pedestrian_data = (p | 'Read pedestrian data' >> beam.io.ReadFromText(pedestrian_data_file)\n",
        "                          | 'JSON_to_Dict pedestrian data' >> beam.ParDo(load_json_as_dictionary())\n",
        "                      )\n",
        "    location_pipeline_name = 'location_data'\n",
        "    location_data = (p | 'Read location data' >> beam.io.ReadFromText(location_data_file)\n",
        "                        | 'JSON_to_Dict location data' >> beam.ParDo(load_json_as_dictionary())\n",
        "                        | 'Select fields location data' >> beam.ParDo(select_elements_location_data())\n",
        "                    )\n",
        "\n",
        "    pipelines_dictionary = {pedestrian_pipeline_name: pedestrian_data,\n",
        "                            location_pipeline_name: location_data}\n",
        "    # print(\"Inside run : pipelines_dictionary\")\n",
        "    # print(pipelines_dictionary)\n",
        "    test_pipeline = (pipelines_dictionary\n",
        "                     | 'Left join' >> LeftJoin(\n",
        "                pedestrian_pipeline_name, pedestrian_data,\n",
        "                location_pipeline_name, location_data, left_key,right_key)\n",
        "                    #  | 'Select fields final data' >> beam.ParDo(drop_redundant_fields())\n",
        "                    #  |beam.io.WriteToText('/content/drive/MyDrive/Colab Notebooks/location_output_data.txt')\n",
        "                     )\n",
        "\n",
        "    output  = ( test_pipeline\n",
        "                | 'format' >> beam.ParDo(drop_redundant_fields())\n",
        "                |beam.io.WriteToText('/location_output_data.txt')\n",
        "    )\n",
        "\n",
        "    result = p.run()\n",
        "    result.wait_until_finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n"
      ],
      "metadata": {
        "id": "9b4YDi6jw7Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RQs-ufDm4Pc"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}